{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ded8ebc-02db-4b91-8c34-16ff3c98b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/static/user/spock/.cache/pypoetry/virtualenvs/ai-iwSNfyWa-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/static/user/spock/.cache/pypoetry/virtualenvs/ai-iwSNfyWa-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.08k/6.08k [00:00<00:00, 8.82MB/s]\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 885/885 [00:00<00:00, 3.89MB/s]\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55.0M/55.0M [00:01<00:00, 38.7MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 388/388 [00:00<00:00, 2.54MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 239k/239k [00:00<00:00, 2.92MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 536kB/s]\n",
      "Downloading builder script: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.55k/4.55k [00:00<00:00, 12.2MB/s]\n",
      "Downloading metadata: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.16k/1.16k [00:00<00:00, 6.01MB/s]\n",
      "Downloading readme: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.30k/4.30k [00:00<00:00, 15.2MB/s]\n",
      "Downloading data files:   0%|                                                                                                                                                                                     | 0/4 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                                                                                     | 0.00/53.4M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|▏                                                                                                                                                                            | 52.2k/53.4M [00:00<02:05, 426kB/s]\u001b[A\n",
      "Downloading data:   1%|▉                                                                                                                                                                            | 307k/53.4M [00:00<00:38, 1.37MB/s]\u001b[A\n",
      "Downloading data:   3%|████▎                                                                                                                                                                       | 1.34M/53.4M [00:00<00:11, 4.47MB/s]\u001b[A\n",
      "Downloading data:   8%|█████████████▍                                                                                                                                                              | 4.18M/53.4M [00:00<00:03, 12.6MB/s]\u001b[A\n",
      "Downloading data:  15%|██████████████████████████▎                                                                                                                                                 | 8.17M/53.4M [00:00<00:02, 21.6MB/s]\u001b[A\n",
      "Downloading data:  25%|██████████████████████████████████████████▉                                                                                                                                 | 13.3M/53.4M [00:00<00:01, 31.1MB/s]\u001b[A\n",
      "Downloading data:  37%|████████████████████████████████████████████████████████████████▍                                                                                                           | 20.0M/53.4M [00:00<00:00, 41.7MB/s]\u001b[A\n",
      "Downloading data:  50%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                      | 26.6M/53.4M [00:00<00:00, 49.2MB/s]\u001b[A\n",
      "Downloading data:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                     | 31.9M/53.4M [00:00<00:00, 50.1MB/s]\u001b[A\n",
      "Downloading data:  69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                     | 37.0M/53.4M [00:01<00:00, 43.4MB/s]\u001b[A\n",
      "Downloading data:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 41.5M/53.4M [00:01<00:00, 43.3MB/s]\u001b[A\n",
      "Downloading data:  88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 47.0M/53.4M [00:01<00:00, 44.0MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53.4M/53.4M [00:01<00:00, 35.7MB/s]\u001b[A\n",
      "Downloading data files:  25%|███████████████████████████████████████████▎                                                                                                                                 | 1/4 [00:02<00:07,  2.51s/it]\n",
      "Downloading data:   0%|                                                                                                                                                                                     | 0.00/13.4M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|▋                                                                                                                                                                            | 52.2k/13.4M [00:00<00:30, 440kB/s]\u001b[A\n",
      "Downloading data:   2%|██▋                                                                                                                                                                           | 207k/13.4M [00:00<00:14, 908kB/s]\u001b[A\n",
      "Downloading data:   3%|████▌                                                                                                                                                                        | 351k/13.4M [00:00<00:12, 1.02MB/s]\u001b[A\n",
      "Downloading data:   4%|██████▉                                                                                                                                                                      | 539k/13.4M [00:00<00:09, 1.29MB/s]\u001b[A\n",
      "Downloading data:   5%|█████████▍                                                                                                                                                                   | 728k/13.4M [00:00<00:09, 1.29MB/s]\u001b[A\n",
      "Downloading data:   7%|████████████                                                                                                                                                                 | 935k/13.4M [00:00<00:08, 1.41MB/s]\u001b[A\n",
      "Downloading data:   9%|██████████████▊                                                                                                                                                             | 1.16M/13.4M [00:00<00:07, 1.60MB/s]\u001b[A\n",
      "Downloading data:  11%|██████████████████▏                                                                                                                                                         | 1.42M/13.4M [00:01<00:07, 1.68MB/s]\u001b[A\n",
      "Downloading data:  13%|█████████████████████▊                                                                                                                                                      | 1.70M/13.4M [00:01<00:06, 1.84MB/s]\u001b[A\n",
      "Downloading data:  15%|█████████████████████████▊                                                                                                                                                  | 2.02M/13.4M [00:01<00:05, 2.04MB/s]\u001b[A\n",
      "Downloading data:  18%|██████████████████████████████▎                                                                                                                                             | 2.36M/13.4M [00:01<00:04, 2.25MB/s]\u001b[A\n",
      "Downloading data:  20%|███████████████████████████████████▏                                                                                                                                        | 2.75M/13.4M [00:01<00:04, 2.48MB/s]\u001b[A\n",
      "Downloading data:  24%|████████████████████████████████████████▌                                                                                                                                   | 3.16M/13.4M [00:01<00:03, 2.74MB/s]\u001b[A\n",
      "Downloading data:  27%|██████████████████████████████████████████████▎                                                                                                                             | 3.62M/13.4M [00:01<00:03, 3.00MB/s]\u001b[A\n",
      "Downloading data:  31%|████████████████████████████████████████████████████▊                                                                                                                       | 4.12M/13.4M [00:01<00:02, 3.29MB/s]\u001b[A\n",
      "Downloading data:  35%|███████████████████████████████████████████████████████████▉                                                                                                                | 4.68M/13.4M [00:02<00:02, 3.64MB/s]\u001b[A\n",
      "Downloading data:  40%|███████████████████████████████████████████████████████████████████▉                                                                                                        | 5.31M/13.4M [00:02<00:02, 4.03MB/s]\u001b[A\n",
      "Downloading data:  45%|████████████████████████████████████████████████████████████████████████████▋                                                                                               | 5.98M/13.4M [00:02<00:01, 4.38MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████████████████████████████████████████████████████████████████████████████████▏                                                                                     | 6.73M/13.4M [00:02<00:01, 4.89MB/s]\u001b[A\n",
      "Downloading data:  56%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                           | 7.55M/13.4M [00:02<00:01, 5.61MB/s]\u001b[A\n",
      "Downloading data:  63%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                | 8.42M/13.4M [00:02<00:00, 6.39MB/s]\u001b[A\n",
      "Downloading data:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 9.08M/13.4M [00:02<00:00, 6.28MB/s]\u001b[A\n",
      "Downloading data:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 10.0M/13.4M [00:02<00:00, 6.70MB/s]\u001b[A\n",
      "Downloading data:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 11.2M/13.4M [00:02<00:00, 7.46MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13.4M/13.4M [00:03<00:00, 4.25MB/s]\u001b[A\n",
      "Downloading data files:  50%|██████████████████████████████████████████████████████████████████████████████████████▌                                                                                      | 2/4 [00:06<00:07,  3.59s/it]\n",
      "Downloading data:   0%|                                                                                                                                                                                     | 0.00/26.5M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|▍                                                                                                                                                                            | 61.4k/26.5M [00:00<00:52, 506kB/s]\u001b[A\n",
      "Downloading data:   1%|█▉                                                                                                                                                                           | 305k/26.5M [00:00<00:19, 1.38MB/s]\u001b[A\n",
      "Downloading data:   5%|████████▋                                                                                                                                                                   | 1.33M/26.5M [00:00<00:05, 4.52MB/s]\u001b[A\n",
      "Downloading data:  18%|███████████████████████████████                                                                                                                                             | 4.79M/26.5M [00:00<00:01, 14.9MB/s]\u001b[A\n",
      "Downloading data:  37%|████████████████████████████████████████████████████████████████                                                                                                            | 9.85M/26.5M [00:00<00:00, 26.7MB/s]\u001b[A\n",
      "Downloading data:  62%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                 | 16.4M/26.5M [00:00<00:00, 39.0MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.5M/26.5M [00:00<00:00, 31.4MB/s]\u001b[A\n",
      "Downloading data files:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                           | 3/4 [00:08<00:02,  2.91s/it]\n",
      "Downloading data:   0%|                                                                                                                                                                                     | 0.00/9.66M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|██                                                                                                                                                                            | 116k/9.66M [00:00<00:11, 826kB/s]\u001b[A\n",
      "Downloading data:   6%|██████████▉                                                                                                                                                                  | 607k/9.66M [00:00<00:03, 2.34MB/s]\u001b[A\n",
      "Downloading data:  27%|██████████████████████████████████████████████▎                                                                                                                             | 2.60M/9.66M [00:00<00:00, 7.49MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.66M/9.66M [00:00<00:00, 16.3MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.48s/it]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 2357.01it/s]\n",
      "Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 127656/127656 [00:12<00:00, 10125.91 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31915/31915 [00:03<00:00, 10251.90 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63978/63978 [00:06<00:00, 10230.22 examples/s]\n",
      "Generating balanced_train split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25868/25868 [00:02<00:00, 10246.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from trl.import_utils import is_xpu_available\n",
    "\n",
    "\n",
    "toxicity = evaluate.load(\"ybelkada/toxicity\", \"DaNLP/da-electra-hatespeech-detection\", module_type=\"measurement\")\n",
    "ds = load_dataset(\"OxAISH-AL-LLM/wiki_toxic\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21895867-153e-4913-b0fc-da8efa5c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Evaluate de-toxified models\")\n",
    "# parser.add_argument(\"--model_type\", default=\"all\", type=str, help=\"Relative path to the source model folder\")\n",
    "parser.add_argument(\"--output_file\", default=\"toxicity.csv\", type=str, help=\"Relative path to the source model folder\")\n",
    "parser.add_argument(\"--batch_size\", default=64, type=int, help=\"Batch size\")\n",
    "parser.add_argument(\"--num_samples\", default=400, type=int, help=\"Number of samples\")\n",
    "parser.add_argument(\"--context_length\", default=2000, type=int, help=\"Number of samples\")\n",
    "parser.add_argument(\"--max_new_tokens\", default=30, type=int, help=\"Max new tokens for generation\")\n",
    "args = parser.parse_args([])#['--model_type',\"EleutherAI/gpt-neo-125M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb065f92-b90d-4e34-9c45-04fbd8fc1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tpye = \"EleutherAI/gpt-neo-125M\"\n",
    "model_type = \"../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M'\" #.127 +- .261\n",
    "model_type = \"../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M/v1\" #.127 +- .261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "412dc070-3f2f-4e05-92cb-840acca19dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_TEST = [model_type]\n",
    "NUM_SAMPLES = args.num_samples\n",
    "BATCH_SIZE = args.batch_size\n",
    "output_file = args.output_file\n",
    "max_new_tokens = args.max_new_tokens\n",
    "context_length = args.context_length\n",
    "if is_xpu_available():\n",
    "    device = torch.xpu.current_device()\n",
    "else:\n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fb2c916-a574-40ef-8675-d537548e3959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\tmodel.safetensors\t tokenizer.json\n",
      "generation_config.json\tspecial_tokens_map.json  vocab.json\n",
      "merges.txt\t\ttokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95c69998-277a-476f-b899-315f4d12103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = AutoModelForCausalLM.from_pretrained(\"../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27d42598-e3dc-4b67-8eaa-2b51f6023319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6243/6243 [00:00<00:00, 79526.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.filter(lambda x: x['label'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14393245-031c-49fa-8069-be67ad2ae4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M/v1 were not used when initializing GPTNeoForCausalLM: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "toxicies = {}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\": device}, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "input_texts = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2f168-65f8-4371-b025-bb3374082da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(ds):\n",
    "    # set seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    input_text = example[\"comment_text\"]\n",
    "    input_texts.append(input_text[:2000])\n",
    "\n",
    "    if i > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    if (i + 1) % BATCH_SIZE == 0:\n",
    "        inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        inputs.input_ids = inputs.input_ids[:context_length]\n",
    "        inputs.attention_mask = inputs.attention_mask[:context_length]\n",
    "        outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        generated_texts = [\n",
    "            generated_text.replace(input_texts[i], \"\") for i, generated_text in enumerate(generated_texts)\n",
    "        ]\n",
    "        toxicity_score = toxicity.compute(predictions=generated_texts)\n",
    "        input_texts = []\n",
    "\n",
    "        if model_id not in toxicities:\n",
    "            toxicities[model_id] = []\n",
    "        toxicities[model_id].extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "# last batch\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=30)\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "generated_texts = [generated_text.replace(input_texts[i], \"\") for i, generated_text in enumerate(generated_texts)]\n",
    "toxicity_score = toxicity.compute(predictions=generated_texts)\n",
    "toxicities[model_id].extend(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373cfbdd-763d-4c1e-a0ef-53913082d8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd0acd-7cdf-4fc5-b0e4-41dd2ee44186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb670d79-4362-4fdb-b38b-fa2b1fcb52d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6243/6243 [00:00<00:00, 82818.19 examples/s]\n",
      "  0%|                                                                                                                                                                                                             | 0/1 [00:00<?, ?it/s]Some weights of the model checkpoint at ../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M/v1 were not used when initializing GPTNeoForCausalLM: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ../../ai/projects/toxicity/output/EleutherAI/gpt-neo-125M/v1 - Mean: 0.1653343452004871 - Std: 0.29652875207280877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# consider only toxic prompts\n",
    "ds = ds.filter(lambda x: x[\"label\"] == 1)\n",
    "\n",
    "toxicities = {}\n",
    "\n",
    "# open a csv file\n",
    "file = open(f\"{output_file}\", \"w\", newline=\"\")\n",
    "writer = csv.writer(file)\n",
    "# add first rows\n",
    "writer.writerow([\"model_id\", \"mean_toxicity\", \"std_toxicity\"])\n",
    "\n",
    "\n",
    "for model_id in tqdm(MODELS_TO_TEST):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\": device}, torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    input_texts = []\n",
    "\n",
    "    for i, example in enumerate(ds):\n",
    "        # set seed\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        input_text = example[\"comment_text\"]\n",
    "        input_texts.append(input_text[:2000])\n",
    "\n",
    "        if i > NUM_SAMPLES:\n",
    "            break\n",
    "\n",
    "        if (i + 1) % BATCH_SIZE == 0:\n",
    "            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "            inputs.input_ids = inputs.input_ids[:context_length]\n",
    "            inputs.attention_mask = inputs.attention_mask[:context_length]\n",
    "            outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "            generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            generated_texts = [\n",
    "                generated_text.replace(input_texts[i], \"\") for i, generated_text in enumerate(generated_texts)\n",
    "            ]\n",
    "            toxicity_score = toxicity.compute(predictions=generated_texts)\n",
    "            input_texts = []\n",
    "\n",
    "            if model_id not in toxicities:\n",
    "                toxicities[model_id] = []\n",
    "            toxicities[model_id].extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # last batch\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=30)\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    generated_texts = [generated_text.replace(input_texts[i], \"\") for i, generated_text in enumerate(generated_texts)]\n",
    "    toxicity_score = toxicity.compute(predictions=generated_texts)\n",
    "    toxicities[model_id].extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # compute mean & std using np\n",
    "    mean = np.mean(toxicities[model_id])\n",
    "    std = np.std(toxicities[model_id])\n",
    "\n",
    "    # save to file\n",
    "    writer.writerow([model_id, mean, std])\n",
    "\n",
    "    # print\n",
    "    print(f\"Model: {model_id} - Mean: {mean} - Std: {std}\")\n",
    "\n",
    "    model = None\n",
    "    if is_xpu_available():\n",
    "        torch.xpu.empty_cache()\n",
    "    else:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# close file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918097b-c6a5-4745-bad4-ffcd44c4b9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802df6f-6c80-4d52-9cbd-de2df571c04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
