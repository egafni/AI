{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c685ddf1-f37d-4e77-ad35-e5606a02320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[2,0,0],\n",
    "              [1,2,1],\n",
    "              [-1,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11b5f41c-39be-47d2-9bd1-2fb1750c56c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.70710678],\n",
       "       [ 1.        , -0.70710678,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(a).eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b15f3df-7241-4a72-b45f-836ef6df0fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u = np.array([1,2,3])\n",
    "u @ u\n",
    "1*1+2*2+3*3\n",
    "np.linalg.norm(u)\n",
    "np.sqrt(1**2+2**2+3**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a44a19c-a75e-45ba-b93d-d9d5eccddfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= np.array([\n",
    "    [5,2,3],\n",
    "    [-1,-3,2],\n",
    "    [0,1,-1]\n",
    "])\n",
    "B = np.array([\n",
    "    [1,0,-4],\n",
    "    [2,1,0],\n",
    "    [8,-1,0]\n",
    "])\n",
    "C = A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b80c245e-8978-47b1-891b-f61ce1b309e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.99999999999998"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6bc3c11-8618-4ce0-9806-ebf23dcdaa5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811538063430.003"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A= np.array([[33, -1, -20],\n",
    "[9,-5,4],\n",
    "[-6,2,0]])\n",
    "1/np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d2a5c7d-0448-4b59-89f5-76669592dddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0020833333333333316"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b65f2a88-0fb9-41ea-88a0-0351560e4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# math.cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de2dc368-33f7-4cf5-a61b-ce4d443f8e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npu@u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bff52d6-45fc-487f-8ff8-0d42a32169f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eadb68c1-f20d-4189-9924-5a6f89d94c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "969634fd-fe73-4698-9513-4075bf6ec4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with PPO\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode\n",
    "    # models like gpt-neo* models are more suitable.\n",
    "    model_name: Optional[str] = field(default=\"distilgpt2\", metadata={\"help\": \"the model name\"})\n",
    "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={\"help\": \"the learning rate\"})\n",
    "    mini_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"the PPO minibatch size\"})\n",
    "    batch_size: Optional[int] = field(default=16, metadata={\"help\": \"the batch size\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    model_save_path: Optional[str] = field(\n",
    "        default=\"./gpt-j-6B-detoxified-long-context-26-shl-1e4-final\",\n",
    "        metadata={\"help\": \"the path to save the model\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30a6bd89-a73a-47d1-9e92-62a9ed41199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args = parser.parse_args_into_dataclasses([])[0]\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=script_args.model_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    ppo_epochs=100,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    batch_size=script_args.batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "320ffab8-d786-4d1e-ad6d-2aaa02224564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with=None, task_name=None, model_name='distilgpt2', query_dataset=None, reward_model=None, remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=2.94e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=16, forward_batch_size=None, mini_batch_size=4, gradient_accumulation_steps=1, world_size=None, ppo_epochs=100, max_grad_norm=None, optimize_cuda_cache=False, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=4, global_backward_batch_size=None, global_batch_size=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84fe39db-646f-4a8b-8e52-cfc5f3b5b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(\n",
    "    config, dataset_name=\"allenai/real-toxicity-prompts\", input_min_text_length=5, input_max_text_length=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    def filter_fn(sample):\n",
    "        toxicity = sample[\"prompt\"][\"toxicity\"]\n",
    "        return toxicity is not None and toxicity > 0.3\n",
    "\n",
    "    ds = ds.filter(filter_fn, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        prompt = sample[\"prompt\"][\"text\"]\n",
    "        continuation = sample[\"continuation\"][\"text\"]\n",
    "\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt + continuation)[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    ds = ds.train_test_split(test_size=0.2, shuffle=False)[\"train\"]\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90bcd371-b944-4b53-b818-2d076847601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "696e4097-d6ed-4373-96ce-bc7779dd815d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer pattern could not be matched.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLMWithValueHead\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# We create a reference model by sharing 20 layers\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m ref_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_reference_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_shared_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# We make sure to use `Adam` optimizer on the model parameters that require gradients.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m/static/user/spock/.cache/pypoetry/virtualenvs/ai-iwSNfyWa-py3.10/lib/python3.10/site-packages/trl/models/modeling_base.py:602\u001b[0m, in \u001b[0;36mcreate_reference_model\u001b[0;34m(model, num_shared_layers, pattern)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer pattern could not be matched.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# divide parameters in shared and unshared parameter lists\u001b[39;00m\n\u001b[1;32m    605\u001b[0m shared_param_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Layer pattern could not be matched."
     ]
    }
   ],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "min_input_length = 30\n",
    "max_input_length = 40\n",
    "dataset = build_dataset(config, input_min_text_length=min_input_length, input_max_text_length=max_input_length)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer. We first load the model\n",
    "# in bfloat16 to save memory using `transformers`.\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)\n",
    "# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "\n",
    "# We create a reference model by sharing 20 layers\n",
    "ref_model = create_reference_model(model, num_shared_layers=6)\n",
    "\n",
    "# We make sure to use `Adam` optimizer on the model parameters that require gradients.\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
    "\n",
    "# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n",
    "# only for this model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# We then build the reward pipeline, we will use the toxicity model to compute the reward.\n",
    "# We first load the toxicity model and tokenizer.\n",
    "toxicity_model_id = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)\n",
    "# We load the toxicity model in fp16 to save memory.\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(\n",
    "    ppo_trainer.accelerator.device\n",
    ")\n",
    "\n",
    "\n",
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "output_min_length = 20\n",
    "output_max_length = 30\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "model_save_path = script_args.model_save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1da03fb-37ec-462b-8379-45a12185a176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.pretrained_model.transformer.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f271882-32de-431e-a035-4c0547b5e244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function trl.models.modeling_base.create_reference_model(model: trl.models.modeling_base.PreTrainedModelWrapper, num_shared_layers: int = None, pattern: str = None) -> trl.models.modeling_base.PreTrainedModelWrapper>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_reference_model(pattern=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3846c3-62d2-48d8-bef4-b7edeee96a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
