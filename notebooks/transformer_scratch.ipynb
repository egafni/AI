{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce889a0-4528-4ba8-a0ba-0624acd81951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.datamodules.text import TextDataModule\n",
    "from ai.constants import REPO_DIR\n",
    "from ai.misc_utils import get_vocab_size\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from ai.models.nlp.transformer import Transformer as Transformer2\n",
    "fname = f'{REPO_DIR}/data/shakespeare.txt'\n",
    "# fname = f'{REPO_DIR}/data/names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a2ba374-6511-415b-9230-f0a9a0b6ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Transformer2.Config(\n",
    "    vocab_size = get_vocab_size(fname),\n",
    "    block_size = 32,\n",
    "    # mlp_size:int = 64\n",
    "    n_embd = 256,\n",
    "    n_heads = 4,\n",
    "    n_blocks = 4,\n",
    "    dropout=0\n",
    ")\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b103fd19-c3ce-40b5-83ce-de22c884486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-08 14:12:17.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mai.datamodules.text\u001b[0m:\u001b[36mprepare_data\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreating datasets...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dm = TextDataModule.Config(fname=fname, block_size=config.block_size, batch_size=batch_size, \n",
    "                           num_workers=0).i()\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad8a4475-8e25-450a-80a3-87afde21b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        c = config\n",
    "        self.config = config\n",
    "        self.q = nn.Linear(c.n_embd, c.n_embd)\n",
    "        self.k = nn.Linear(c.n_embd, c.n_embd)\n",
    "        self.v = nn.Linear(c.n_embd, c.n_embd)\n",
    "\n",
    "        attn_mask = torch.ones(c.block_size, c.block_size)\n",
    "        attn_mask.masked_fill(~torch.tril(attn_mask).to(bool), -torch.inf)\n",
    "        self.register_buffer('attn_mask', attn_mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c = self.config\n",
    "        B=x.shape[0]\n",
    "        T=c.block_size\n",
    "        C=c.n_embd\n",
    "        nh=c.n_heads\n",
    "\n",
    "        \n",
    "        q=self.q(x) # B,T,C\n",
    "        k=self.k(x) # B,T,C\n",
    "        v=self.v(x) # B,T,C\n",
    "        q=q.view(B,T,nh,C//nh).permute(0,2,1,3) # B,nh,T,C//nh\n",
    "        k=k.view(B,T,nh,C//nh).permute(0,2,3,1) # B,nh,C//nh,T\n",
    "        v=v.view(B,T,nh,C//nh).permute(0,2,1,3) # B,nh,T,C//nh\n",
    "        \n",
    "        attn = q@k * (math.sqrt(1/(C//nh))) # B,nh,T,T\n",
    "        attn *= self.attn_mask\n",
    "        attn = torch.softmax(attn, dim=-1) # B,nh,T,T\n",
    "        o = attn@v # B,nh,T,C//nh\n",
    "        \n",
    "        return o.permute(0,2,1,3).contiguous().view(B,T,C)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        c = config\n",
    "        self.l1 = nn.Linear(c.n_embd, c.n_embd*4)\n",
    "        self.l2 = nn.Linear(c.n_embd*4, c.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = self.l1(x)\n",
    "        o = torch.relu(o)\n",
    "        o = self.l2(o)\n",
    "        return o\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        c = config\n",
    "        self.config = config\n",
    "\n",
    "        self.attn = Attention(config)\n",
    "        self.ln1 = nn.LayerNorm(c.n_embd)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(c.n_embd)\n",
    "        \n",
    "        self.ff = FeedForward(c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = x + self.attn(self.ln1(x))\n",
    "        o = x + self.ff(self.ln2(o))\n",
    "        return o\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        c = config\n",
    "        self.config=config\n",
    "        self.token_emb = nn.Embedding(c.vocab_size, c.n_embd)\n",
    "        self.pos_emb = nn.Embedding(c.block_size, c.n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(c.n_blocks)])\n",
    "        self.ln_final = nn.LayerNorm(c.n_embd)\n",
    "        self.proj = nn.Linear(c.n_embd, c.vocab_size)\n",
    "        \n",
    "        self.register_buffer('pos_id', torch.arange(0, c.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(self.pos_id)\n",
    "        o = pos_emb + tok_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            o = block(o)\n",
    "        self.ln_final(o)\n",
    "        o = self.proj(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "# B = batch_size\n",
    "T = config.block_size\n",
    "model = Transformer(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8614e6e-0bed-4286-a8a5-fd61b415d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb1ff0-d6ef-4f85-84d5-683a0d275858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e0|s100: loss=3.03\n",
      "e0|s200: loss=2.84\n"
     ]
    }
   ],
   "source": [
    "lr=.01\n",
    "\n",
    "step=0\n",
    "for epoch in range(4):\n",
    "    for X,y in iter(dl):\n",
    "        step+=1\n",
    "        B = X.shape[0]\n",
    "    \n",
    "        # forward\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits.view(B*T,config.vocab_size), y.view(B*T))\n",
    "        if step % 100 == 0:\n",
    "            print(f'e{epoch}|s{step}: loss={loss:.2f}')\n",
    "    \n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        loss.backward()\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.data -= param.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9797c67f-9667-42dc-b5c3-7d53dd45e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r'd he hath not,\n",
      "But basely yiel\n",
      "************************************************************************\n",
      " WbawsriositdhnrehywkoBn,.Aawaa\n",
      "Xhydd Sk e aa:,wf antus;tn  e nmstrhlnygh  da emnattedrv,hhDmrur aghVsrfn? hdrnofpG syrht  hou tPyo fe Mwprl\n",
      " tc olIdhHgw.,\n",
      " ecrMeosVteBaauVsUo:'npw o  :n t    g. wnsef f ddi oua , qNlMasSh fo oensT d\n",
      "o\n",
      " ofratFrfols  ltdhU e\n"
     ]
    }
   ],
   "source": [
    "x = X.clone()\n",
    "result = []\n",
    "for i in range(30):\n",
    "    logits = model(x)\n",
    "    probas = torch.softmax(logits, -1)\n",
    "    # greedy\n",
    "    # token_ids = probas[:,-1].argmax(-1)\n",
    "    token_ids = torch.multinomial(probas[:,-1], num_samples=1)\n",
    "    x = x.roll(-1)\n",
    "    x[:,-1] = token_ids.flatten()\n",
    "    result.append(token_ids.flatten())\n",
    "\n",
    "\n",
    "prompt = X[0]\n",
    "answer = result[0]\n",
    "print(''.join(dm.ds_train.decode(prompt)))\n",
    "print('*'*72)\n",
    "print(''.join(dm.ds_train.decode(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b8b6ab0-6ac5-46b2-b04c-e6cdcc9b49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.ds_train.decode(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43a6f8-23fa-4d38-82ff-5ea6b61edb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
