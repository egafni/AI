{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4ab5410-0fdd-4899-af70-ebba8641fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\"\"\"\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\"\"\"\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9d25757-5815-4a28-9a3e-a80ed6bc71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "train_dataloader = Multi30k(split='train', language_pair=('de','en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ce31fe7-96b4-4fa5-807d-de5816e1bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc8165fe-ffa4-4c44-b554-e103f2ffd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "de,en = zip(*train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e525758c-d351-43c7-91de-256005ca5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, sentences, max_len):\n",
    "        words = ['<EOS>','SOS']\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.strip().split():\n",
    "                words.append(word)\n",
    "        self.i_to_t = {i:t for i,t in enumerate(words)}\n",
    "        self.t_to_i = {t:i for i,t in enumerate(words)}\n",
    "        self.vocab_size = len(self.i_to_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6bc93755-267c-4038-a6b7-c4c576e2ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05f0ce1c-a859-4325-9c75-7c86b48a3095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101668/101668 [00:08<00:00, 12609.07 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25417/25417 [00:02<00:00, 12351.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "# prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=64, truncation=True)\n",
    "    return model_inputs\n",
    "    \n",
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2c4aa77-95cf-4e6b-b4cd-83bc2faba216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 101668\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_books['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb5f7d47-6cc9-431a-bfb0-1e8974aedc6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'translation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenized_books\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd3/user/spock/.cache/pypoetry/virtualenvs/ai-iwSNfyWa-py3.10/lib/python3.10/site-packages/datasets/dataset_dict.py:74\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     78\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'translation'"
     ]
    }
   ],
   "source": [
    "tokenized_books['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1b007-552d-48b9-9948-8b6f72e4bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78daab-15bc-440d-ae3e-878f6c3ee702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        # decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, x, hidden):\n",
    "        output = self.embedding(x)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e44e2-8d42-4eb2-8c71-6f2c369e3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "input_size = input_lang.n_words\n",
    "hidden_size = 64\n",
    "dropout=0.0\n",
    "output_size = output_lang.n_words\n",
    "encoder = EncoderRNN(input_size, hidden_size,dropout_p=dropout).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size).to(device)\n",
    "\n",
    "lr = 1e-2\n",
    "optim = AdamW(params=encoder.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e9016-512a-40b4-add3-5f191c05bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for epoch in range(1):\n",
    "    for X,y in train_dataloader:\n",
    "        step +=1\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        enc_outputs, enc_hidden = encoder(X)\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(enc_outputs, enc_hidden, target_tensor=y)\n",
    "        loss = F.cross_entropy(decoder_outputs.view(-1, decoder_outputs.shape[-1]), y.view(-1))\n",
    "        if step % 100 == 0:\n",
    "            print(f'e{epoch}:s{step} | loss:{loss:.3f}')\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.grad = 0.0\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        # optim.step()\n",
    "        for param in model.parameters():\n",
    "            param.data -= param.grad * lr\n",
    "\n",
    "\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1e1b64e3-2847-4b94-b967-2ded9d0d5700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> what is that thing ?\n",
      "= que es esa cosa ?\n",
      "< refractor refractor refractor atenas abotonarme ram solida hablanos aparentar escribia\n",
      "\n",
      "> tom lived in a trailer\n",
      "= tom vivia en una caravana\n",
      "< pudiera pudiera pudiera caigo partiste ahuyentar darselo escribia mary rascacielos\n",
      "\n",
      "> people sometimes make illogical decisions\n",
      "= a veces las personas toman decisiones ilogicas\n",
      "< la quedaban captura dejan tocarme prefieres aventuroso trasplante cuidar llamarlo\n",
      "\n",
      "> i went into details\n",
      "= entre en detalles\n",
      "< te confundio evacuar chance peligros traerse salta saliera copenhague conseguira\n",
      "\n",
      "> she went blind\n",
      "= se quedo ciega\n",
      "< ella atenas atenas viejas polonia escribia formalidad enigma alimenticios moderese\n",
      "\n",
      "> he is soon to be a father\n",
      "= dentro de poco sera padre\n",
      "< el el el honestamente viejas copenhague nacimientos peligros apartame hablanos\n",
      "\n",
      "> i got here a little early today\n",
      "= llegue aqui un poco temprano hoy\n",
      "< pensaste refractor atenas viejas polonia escribia tengamos copenhague nacimientos peligros\n",
      "\n",
      "> whose is this bag ?\n",
      "= de quien es esta mochila ?\n",
      "< resguardado llevaron refractor refractor parezco ahuyentar permaneced desafiaron ceremonias mahjongg\n",
      "\n",
      "> tom walked through the town\n",
      "= tom camino a traves de la ciudad\n",
      "< pudiera pudiera pudiera caigo partiste trasplante pachanga alegaciones solida firmeza\n",
      "\n",
      "> i believe you ll get over it soon\n",
      "= creo que te recuperaras pronto\n",
      "< ceremonias atenas atenas atenas vacante generalidad viejas azucar budapest huye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "evaluateRandomly(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3ad3b-0acf-4b3f-a32f-a3ae6e247239",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
