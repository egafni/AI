{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640e16e0-1338-4bc9-bc7a-3eae64a900e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain = \"docs.llamaindex.ai\"\n",
    "# docs_url = \"https://docs.llamaindex.ai/en/latest/\"\n",
    "# !wget -e robots=off --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains {domain} --no-parent {docs_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d33c35-760a-4cb9-92ee-b0a16e5b8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import UnstructuredReader\n",
    "\n",
    "reader = UnstructuredReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c12d49-e207-4de8-b027-1c6a1a098fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
    "all_files = [f.resolve() for f in all_files_gen]\n",
    "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]\n",
    "len(all_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa90d1c1-9901-4268-9de5-ceb04ef6fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0/1206\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdx \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_html_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m loaded_docs \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241m.\u001b[39mload_data(file\u001b[38;5;241m=\u001b[39mf, split_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Hardcoded Index. Everything before this is ToC for all pages\u001b[39;00m\n\u001b[1;32m     13\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m72\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reader' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# TODO: set to higher value if you want more docs\n",
    "doc_limit = 100\n",
    "\n",
    "docs = []\n",
    "for idx, f in enumerate(all_html_files):\n",
    "    if idx > doc_limit:\n",
    "        break\n",
    "    print(f\"Idx {idx}/{len(all_html_files)}\")\n",
    "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
    "    # Hardcoded Index. Everything before this is ToC for all pages\n",
    "    start_idx = 72\n",
    "    loaded_doc = Document(\n",
    "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[72:]]),\n",
    "        metadata={\"path\": str(f)},\n",
    "    )\n",
    "    print(loaded_doc.metadata[\"path\"])\n",
    "    docs.append(loaded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa5732-46e9-4709-ad0b-37118e557a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a71b8-5ab2-45f5-938e-7c1ccb442b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "async def build_agent_per_doc(nodes, file_base):\n",
    "    print(file_base)\n",
    "\n",
    "    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\n",
    "    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\n",
    "    if not os.path.exists(vi_out_path):\n",
    "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\", llm=llm\n",
    "    )\n",
    "\n",
    "    # extract a summary\n",
    "    if not os.path.exists(summary_out_path):\n",
    "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        summary = str(\n",
    "            await summary_query_engine.aquery(\n",
    "                \"Extract a concise 1-2 line summary of this document\"\n",
    "            )\n",
    "        )\n",
    "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
    "    else:\n",
    "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"vector_tool_{file_base}\",\n",
    "                description=f\"Useful for questions related to specific facts\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"summary_tool_{file_base}\",\n",
    "                description=f\"Useful for summarization questions\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return agent, summary\n",
    "\n",
    "\n",
    "async def build_agents(docs):\n",
    "    node_parser = SentenceSplitter()\n",
    "\n",
    "    # Build agents dictionary\n",
    "    agents_dict = {}\n",
    "    extra_info_dict = {}\n",
    "\n",
    "    # # this is for the baseline\n",
    "    # all_nodes = []\n",
    "\n",
    "    for idx, doc in enumerate(tqdm(docs)):\n",
    "        nodes = node_parser.get_nodes_from_documents([doc])\n",
    "        # all_nodes.extend(nodes)\n",
    "\n",
    "        # ID will be base + parent\n",
    "        file_path = Path(doc.metadata[\"path\"])\n",
    "        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\n",
    "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
    "\n",
    "        agents_dict[file_base] = agent\n",
    "        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\n",
    "\n",
    "    return agents_dict, extra_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927dcee6-6fea-4dd7-ae2b-56bc69975c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agents_dict, extra_info_dict = await build_agents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db3a9b-2b47-4890-80e6-84c6adb11995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for file_base, agent in agents_dict.items():\n",
    "    summary = extra_info_dict[file_base][\"summary\"]\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agent,\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{file_base}\",\n",
    "            description=summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)\n",
    "print(all_tools[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a2ea5-2d0c-4b49-80d1-c124e230dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import (\n",
    "    ObjectIndex,\n",
    "    SimpleToolNodeMapping,\n",
    "    ObjectRetriever,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4-0613\")\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10)\n",
    "\n",
    "\n",
    "# define a custom retriever with reranking\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, postprocessor=None):\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._postprocessor = postprocessor or CohereRerank(top_n=5)\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        filtered_nodes = self._postprocessor.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle=query_bundle\n",
    "        )\n",
    "\n",
    "        return filtered_nodes\n",
    "\n",
    "\n",
    "# define a custom object retriever that adds in a query planning tool\n",
    "class CustomObjectRetriever(ObjectRetriever):\n",
    "    def __init__(self, retriever, object_node_mapping, all_tools, llm=None):\n",
    "        self._retriever = retriever\n",
    "        self._object_node_mapping = object_node_mapping\n",
    "        self._llm = llm or OpenAI(\"gpt-4-0613\")\n",
    "\n",
    "    def retrieve(self, query_bundle):\n",
    "        nodes = self._retriever.retrieve(query_bundle)\n",
    "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
    "\n",
    "        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "            query_engine_tools=tools, llm=self._llm\n",
    "        )\n",
    "        sub_question_description = f\"\"\"\\\n",
    "Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\n",
    "tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n",
    "\"\"\"\n",
    "        sub_question_tool = QueryEngineTool(\n",
    "            query_engine=sub_question_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"compare_tool\", description=sub_question_description\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return tools + [sub_question_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1522bc-6202-4bed-a048-3aff88a48191",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_node_retriever = CustomRetriever(vector_node_retriever)\n",
    "\n",
    "# wrap it with ObjectRetriever to return objects\n",
    "custom_obj_retriever = CustomObjectRetriever(\n",
    "    custom_node_retriever, tool_mapping, all_tools, llm=llm\n",
    ")\n",
    "tmps = custom_obj_retriever.retrieve(\"hello\")\n",
    "print(len(tmps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ceb5b0-c5c6-47f2-aba6-3ec25a3f6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai_legacy import FnRetrieverOpenAIAgent\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "top_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
    "    custom_obj_retriever,\n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries about the documentation.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# top_agent = ReActAgent.from_tools(\n",
    "#     tool_retriever=custom_obj_retriever,\n",
    "#     system_prompt=\"\"\" \\\n",
    "# You are an agent designed to answer queries about the documentation.\n",
    "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "# \"\"\",\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197b325-3445-40a7-87ce-17f87e4835fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = [\n",
    "    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\n",
    "]\n",
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2ca75-4a34-4672-8d23-7a0b152011f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
